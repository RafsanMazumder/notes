# ✅ Kafka Interview Notes: 3. Producers – Acks, Retries, Idempotence

---

## 🔷 Kafka Producer Responsibilities

A Kafka **producer** is responsible for:

* Sending records to Kafka **topics** (and their partitions)
* Optionally **batching**, **retrying**, and **compressing** records
* Managing **delivery guarantees** like at-most-once, at-least-once, and exactly-once

Kafka provides multiple tunable settings to control these aspects.

**Interview insight**: *The producer's configuration directly determines your application's data consistency guarantees.
A misconfigured producer can lead to data loss, duplicates, or poor performance - making this one of the most critical
components to understand.*

---

## 🔸 1. `acks` Configuration

The `acks` parameter controls **when the producer considers a message as "successfully sent"**.

| acks          | Meaning                                                       | Durability         | Performance          |
|---------------|---------------------------------------------------------------|--------------------|----------------------|
| `0`           | Don't wait for any response from broker                       | Low (risk of loss) | High (fire & forget) |
| `1`           | Wait for **leader** broker to persist the record              | Medium             | Medium               |
| `all` or `-1` | Wait for **all in-sync replicas (ISR)** to persist the record | High (durable)     | Lower (slower)       |

**Best practice**: Use `acks=all` for strong delivery guarantees.

### Deep Dive into Acks Settings (Critical Interview Topic):

**`acks=0` (Fire and Forget):**

- Producer doesn't wait for any acknowledgment
- Highest throughput, lowest latency
- *"When would you use acks=0?"* - High-frequency metrics, logs where occasional loss is acceptable, real-time analytics
  where speed matters more than completeness
- **Risk**: Data loss if broker fails before writing to disk
- **Real-world example**: Stock price feeds where the next price update makes the previous one less relevant

**`acks=1` (Leader Acknowledgment):**

- Producer waits for partition leader to write to its local log
- Balanced approach between durability and performance
- *"What's the risk with acks=1?"* - If leader fails after acknowledging but before followers replicate, data is lost
- **Scenario**: Leader crashes immediately after ack but before followers sync → message lost during leader election
- **Use case**: Most general-purpose applications where some data loss is tolerable

**`acks=all` (ISR Acknowledgment):**

- Producer waits for all in-sync replicas to acknowledge
- Strongest durability guarantee
- *"How does min.insync.replicas interact with acks=all?"* - If ISR falls below min.insync.replicas, producer gets
  errors, preventing data loss but sacrificing availability
- **Configuration example**: `min.insync.replicas=2` with `replication.factor=3` allows one replica to be down
- **Use case**: Financial transactions, audit logs, any critical business data

### Advanced Acks Concepts:

**ISR Dynamics with Acks:**

- *"What happens if ISR shrinks while producer is sending with acks=all?"* - Producer starts getting
  `NotEnoughReplicasException`
- *"Should you prefer availability or consistency?"* - Configure based on business needs. Banking = consistency,
  analytics = availability

**Performance Implications:**

- `acks=all` can be 2-10x slower than `acks=1`
- Network partitions can cause significant delays with `acks=all`
- Batching becomes more critical with higher ack requirements

---

## 🔸 2. Retries (`retries`, `retry.backoff.ms`)

Kafka producers **automatically retry** on **transient errors** (e.g., network failures, leader not available):

* `retries` (default: 5) – max number of retry attempts
* `retry.backoff.ms` – wait time between retries
* Retries are **idempotent only if `enable.idempotence=true`**

> ⚠️ **Without idempotence**, retries may result in **duplicate messages**.

### Retry Mechanism Deep Dive:

**Retriable vs Non-Retriable Errors:**

**Retriable Errors (Will be retried):**

- `NotLeaderForPartitionException` - Leader election in progress
- `NetworkException` - Temporary network issues
- `TimeoutException` - Request timeout (broker overloaded)
- `RetriableException` - Generic retriable errors

**Non-Retriable Errors (Will NOT be retried):**

- `RecordTooLargeException` - Message exceeds max size
- `SerializationException` - Data serialization failed
- `InvalidTopicException` - Topic doesn't exist
- `AuthorizationException` - Permission denied

**Advanced Retry Configuration:**

```java
// Modern recommended settings
props.put("retries", Integer.MAX_VALUE);  // Infinite retries
props.put("delivery.timeout.ms", 120000); // 2 minutes total time
props.put("retry.backoff.ms", 100);       // Start with 100ms
props.put("request.timeout.ms", 30000);   // 30 seconds per request
```

**Interview scenarios:**

- *"Why use infinite retries instead of a fixed number?"* - `delivery.timeout.ms` provides overall timeout, so infinite
  retries prevent premature giving up on transient issues
- *"What happens during a long leader election?"* - Producer retries with exponential backoff until
  `delivery.timeout.ms` is reached
- *"How do you handle poison messages that always fail?"* - Use `max.in.flight.requests.per.connection=1` and monitor
  error metrics to detect systematic failures

**Retry Ordering Concerns:**

- Without idempotence, retries can cause message reordering
- *"How can retries break message ordering?"* - If request A fails and request B succeeds, then A retries and succeeds,
  you get B-A order instead of A-B
- Solution: Set `max.in.flight.requests.per.connection=1` or enable idempotence

---

## 🔸 3. Idempotent Producer (`enable.idempotence=true`)

Kafka 0.11+ supports **idempotent producers**, meaning:

> Even if a message is **retried**, it will be written **only once** to the Kafka topic.

### How it works:

* Kafka attaches a **producer ID (PID)** and **sequence number** to each message.
* The broker detects duplicates and filters them out.

### Properties:

* Automatically enabled in Kafka >=2.5 when using `acks=all` and no custom partitioner.
* Must set:

```java
props.put("enable.idempotence", true);
```

| Feature               | Without Idempotence | With Idempotence  |
|-----------------------|---------------------|-------------------|
| Retries cause dupes?  | Yes                 | No (safe retries) |
| Sequence enforced?    | No                  | Yes               |
| Safe for exactly-once | No                  | Yes (partial)     |

### Idempotence Implementation Details (Interview Gold):

**Producer ID (PID) Assignment:**

- Each producer instance gets a unique PID from the broker
- PID is valid for the lifetime of the producer instance
- *"What happens if producer restarts?"* - Gets new PID, so idempotence only works within single producer session

**Sequence Number Mechanism:**

- Each message gets incrementing sequence number per partition
- Broker expects consecutive sequence numbers
- *"What if sequence numbers are out of order?"* - Broker rejects with `OutOfOrderSequenceException`

**Limitations of Idempotence:**

- Only prevents duplicates within single producer session
- Doesn't work across producer restarts
- Limited to 5 in-flight requests per partition
- *"Why is it limited to 5 in-flight requests?"* - Broker needs to buffer messages to detect gaps in sequence numbers

**Configuration Requirements:**

```java
// These are automatically set when idempotence is enabled
props.put("enable.idempotence", true);
props.put("acks", "all");                          // Required
props.put("retries", Integer.MAX_VALUE);           // Required  
props.put("max.in.flight.requests.per.connection", 5); // Max allowed
```

**Real-world scenario:**
*"Your producer sends a message, network times out, producer retries, but original message actually succeeded. Without
idempotence, you get duplicates. With idempotence, broker detects duplicate sequence number and ignores retry."*

---

## 🔸 4. Exactly Once Semantics (EOS)

**Exactly-once** = record is **delivered once**, **processed once**, and **no duplicates** even after retries.

Kafka achieves **EOS** by combining:

1. **Idempotent producer**
2. **Transactions** across partitions/topics

> EOS is **not just a producer setting**, it requires:

* `enable.idempotence=true`
* `transactional.id` property
* Using `initTransactions()`, `beginTransaction()`, `commitTransaction()`

Use case: **Stateful stream processing** or **multi-topic writes** that must be atomic.

### Exactly-Once Deep Dive (Advanced Interview Topic):

**Transaction Coordinator:**

- Special Kafka component that manages transactions
- Maintains transaction state in internal `__transaction_state` topic
- Handles two-phase commit protocol across partitions

**Transactional Producer Lifecycle:**

```java
// 1. Configure transactional producer
props.put("transactional.id", "my-transactional-id");
props.put("enable.idempotence", true);

// 2. Initialize transactions
producer.initTransactions();

// 3. Begin transaction
producer.beginTransaction();

// 4. Send messages (can span multiple topics/partitions)
producer.send(record1);
producer.send(record2);

// 5. Commit or abort
producer.commitTransaction(); // or producer.abortTransaction();
```

**Transaction Guarantees:**

- All messages in transaction are committed together or none are
- Consumers can be configured to read only committed messages
- *"What happens if producer crashes mid-transaction?"* - Transaction coordinator times out the transaction and marks it
  as aborted

**Consumer-side Configuration for EOS:**

```java
props.put("isolation.level", "read_committed"); // Only read committed messages
```

**EOS Limitations and Trade-offs:**

- **Performance**: Transactions add overhead (2-3x slower than non-transactional)
- **Complexity**: More complex error handling and state management
- **Scale**: Limited by transaction coordinator's capacity
- **Cross-cluster**: EOS doesn't work across Kafka clusters

**Real-world EOS scenarios:**

- *"Stream processing application reads from topic A, processes data, writes to topic B. How do you ensure
  exactly-once?"* - Use Kafka Streams or implement custom transactional consumer-producer loop
- *"Database + Kafka updates must be atomic. How?"* - Use distributed transaction patterns or event sourcing with
  compensating actions

---

## 🔸 5. Batching & Performance

Kafka producers can batch multiple records into a **single request** to improve throughput.

| Property           | Description                                                              |
|--------------------|--------------------------------------------------------------------------|
| `batch.size`       | Max size (in bytes) of a single batch per partition                      |
| `linger.ms`        | Time to wait before sending even if batch isn't full                     |
| `compression.type` | gzip/snappy/lz4/zstd – reduces payload size, improves network efficiency |

> Batching + compression = **better performance**, but increases **latency**

### Batching Strategy Deep Dive:

**Batch Formation Logic:**

- Producer maintains separate batch for each partition
- Batch sent when: `batch.size` reached OR `linger.ms` timeout OR buffer full
- *"Why have separate batches per partition?"* - Each partition may be on different brokers, so batching per partition
  optimizes network usage

**Tuning Batch Size:**

- **Too small**: More network requests, lower throughput
- **Too large**: Higher memory usage, increased latency
- **Sweet spot**: Usually 16KB-100KB depending on message size and throughput needs
- *"How do you determine optimal batch size?"* - Monitor batch size metrics and network utilization. Start with 16KB and
  increase if network is underutilized.

**Linger Time Strategy:**

- `linger.ms=0`: Send immediately (lowest latency)
- `linger.ms=5-20`: Good balance for most applications
- `linger.ms=100+`: Optimize for throughput over latency
- *"When would you use high linger times?"* - Batch processing, analytics workloads where latency is less critical than
  throughput

**Compression Trade-offs:**

| Compression | CPU Usage | Compression Ratio | Use Case                  |
|-------------|-----------|-------------------|---------------------------|
| none        | None      | 1:1               | CPU-limited environments  |
| snappy      | Low       | 2-3:1             | Good default choice       |
| lz4         | Low       | 2-3:1             | Fastest compression       |
| gzip        | Medium    | 3-5:1             | Good compression ratio    |
| zstd        | Medium    | 3-6:1             | Best balance (Kafka 2.1+) |

**Advanced Batching Concepts:**

**Buffer Memory Management:**

- `buffer.memory`: Total memory for buffering records (default 32MB)
- When buffer full, `send()` blocks for `max.block.ms`
- *"What happens when producer can't keep up?"* - Buffer fills up, send() blocks, eventually throws timeout exception

**Record Accumulator Pattern:**

- Producer uses single background thread (Sender) for all network I/O
- Application threads only add to batches, don't do network I/O
- *"Why this design?"* - Separates application logic from network concerns, enables efficient batching

---

## 🔸 6. Delivery Guarantees Summary

| Guarantee         | Description                                 | Config Requirements                                       |
|-------------------|---------------------------------------------|-----------------------------------------------------------|
| **At-most-once**  | May lose messages (e.g., crash before send) | `acks=0`, no retries                                      |
| **At-least-once** | Possible duplicates (e.g., retries)         | `acks=1/all`, `retries>0`, no idempotence                 |
| **Exactly-once**  | No loss, no duplicates                      | `acks=all`, `enable.idempotence=true`, `transactional.id` |

### Delivery Guarantees in Practice:

**At-Most-Once Scenarios:**

- High-frequency metrics where loss is acceptable
- Real-time feeds where fresh data matters more than completeness
- *"Risk"*: Data loss if producer or broker fails

**At-Least-Once Scenarios:**

- Most common choice for production systems
- Applications that can handle duplicates (idempotent processing)
- *"Challenge"*: Downstream systems must handle duplicates gracefully

**Exactly-Once Scenarios:**

- Financial transactions, billing systems
- Stream processing with state that can't handle duplicates
- *"Cost"*: Significant performance and complexity overhead

**Choosing the Right Guarantee:**

- *"How do you decide between guarantees?"* - Consider business impact of loss vs duplicates, performance requirements,
  and system complexity tolerance
- *"Can you mix guarantees?"* - Yes, different producers can use different settings based on data criticality

---

## 🔸 Producer Send Path Summary

1. Record is added to **partition-specific batch**.
2. Batch is sent when:
    * `batch.size` is full, or
    * `linger.ms` expires.
3. Broker receives and appends to log.
4. Ack returned (depending on `acks`).
5. Producer retries on failure (if configured).

### Detailed Send Path Analysis:

**Producer-Side Steps:**

1. **Serialization**: Convert key/value objects to bytes
2. **Partitioning**: Determine target partition (hash of key or custom logic)
3. **Batching**: Add to partition-specific batch
4. **Compression**: Compress batch if configured
5. **Network Send**: Sender thread transmits to broker

**Broker-Side Steps:**

1. **Validation**: Check message format, size, permissions
2. **Partition Leader Check**: Ensure this broker is the leader
3. **Log Append**: Write to partition log file
4. **Replication**: Followers fetch and replicate (if acks=all)
5. **Acknowledgment**: Send success/failure response

**Error Handling Flow:**

- Retriable error → Add to retry queue → Retry with backoff
- Non-retriable error → Fail immediately → Call error callback
- Timeout → Fail after `delivery.timeout.ms` → Call error callback

---

## 🔸 Advanced Producer Patterns

### Asynchronous vs Synchronous Sending

**Asynchronous (Recommended):**

```java
producer.send(record, (metadata, exception) -> {
    if (exception != null) {
        // Handle error
    } else {
        // Success - metadata contains offset, partition, etc.
    }
});
```

**Synchronous (Blocking):**

```java
try {
    RecordMetadata metadata = producer.send(record).get();
    // Success
} catch (Exception e) {
    // Handle error
}
```

**Trade-offs:**

- Async: Higher throughput, more complex error handling
- Sync: Simpler error handling, lower throughput

### Custom Partitioners

```java
public class GeographicPartitioner implements Partitioner {
    public int partition(String topic, Object key, byte[] keyBytes,
                        Object value, byte[] valueBytes, Cluster cluster) {
        String region = extractRegion(key);
        return region.equals("US") ? 0 : 1;
    }
}
```

**Use cases:**

- Geographic data distribution
- Tenant isolation in multi-tenant systems
- Load balancing based on business logic

### Producer Interceptors

```java
public class AuditInterceptor implements ProducerInterceptor<String, String> {
    public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
        // Audit, modify, or enrich records before sending
        return record;
    }
    
    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {
        // Handle post-send logic (logging, metrics, etc.)
    }
}
```

---

## 🔸 Common Pitfalls

* **High retries + acks=1 + no idempotence** → duplicates
* **Large batch.size** but no `linger.ms` → underfilled batches
* Forgetting to **commit or abort transactions** → hung producer
* **Too many in-flight requests** (`max.in.flight.requests.per.connection`) can break ordering if retries occur (unless
  idempotence is on)

### Additional Production Pitfalls:

**Memory Leaks:**

- Not closing producers properly in application shutdown
- Creating too many producer instances instead of sharing
- *"Best practice"*: Use singleton producer per application, properly close in shutdown hooks

**Performance Anti-patterns:**

- Using synchronous send in high-throughput scenarios
- Not enabling compression for large messages
- Setting `linger.ms=0` when throughput matters more than latency

**Configuration Mistakes:**

- Using default `batch.size` (16KB) for high-throughput applications
- Not tuning `buffer.memory` for burst traffic
- Enabling transactions without understanding performance impact

**Monitoring Blind Spots:**

- Not monitoring producer metrics (batch size, compression ratio, error rates)
- Ignoring `buffer-exhausted-rate` metric
- Not alerting on high retry rates

---

## 🔸 Production Monitoring

**Key Producer Metrics:**

- `record-send-rate`: Messages per second
- `batch-size-avg`: Average batch size (tune batching)
- `compression-rate-avg`: Compression efficiency
- `record-retry-rate`: Retry frequency (watch for spikes)
- `buffer-exhausted-rate`: Buffer pressure indicator

**Health Checks:**

- Producer response times
- Error rates by error type
- Transaction success/abort ratios (if using EOS)

---

## ❓ Interview Questions

### Basic Level:

1. What are the trade-offs between `acks=1` and `acks=all`?
2. How does Kafka ensure idempotence in producers?
3. How do retries interact with delivery guarantees?
4. How do batching and `linger.ms` impact throughput and latency?
5. Explain how Kafka provides exactly-once semantics.

### Intermediate Level:

6. What happens when a producer's buffer memory is exhausted?
7. How do you handle message ordering with retries enabled?
8. What's the difference between retriable and non-retriable errors?
9. How does compression affect producer performance?
10. When would you use synchronous vs asynchronous sending?

### Advanced Level:

11. Design a producer configuration for a high-throughput, loss-tolerant analytics system.
12. How would you implement exactly-once processing in a microservices architecture?
13. What are the trade-offs of using transactions in Kafka?
14. How do you debug producer performance issues in production?
15. Explain the interaction between idempotence, transactions, and consumer isolation levels.

### System Design Level:

16. Design a producer strategy for a multi-tenant SaaS platform with different SLA requirements.
17. How would you handle producer failover in a distributed system?
18. What's your approach to schema evolution with Kafka producers?
19. How do you ensure data consistency when writing to both Kafka and a database?
20. Design a monitoring and alerting strategy for Kafka producers in production.