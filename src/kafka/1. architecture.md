# ✅ Kafka Interview Notes: 1. Kafka Architecture & Components

---

## 🔷 Overview

**Kafka** is a **distributed event streaming platform** used to build real-time data pipelines and streaming apps. It's
designed for:

* High throughput
* Horizontal scalability
* Durability
* Low latency

Kafka achieves this via its **core architecture** built around brokers, topics, partitions, and replication.

---

## 🔸 Kafka Key Components

### 🔹 1. **Broker**

* A **Kafka broker** is a single Kafka server.
* A Kafka cluster consists of multiple brokers (typically ≥3 for fault tolerance).
* Each broker stores data for one or more **topic partitions**.
* One broker acts as the **controller**, responsible for:
    * Leader election for partitions
    * Detecting broker failures
    * Managing cluster metadata (if ZooKeeper is used)

> **Each broker is identified by a unique ID.** A broker may be a leader for some partitions and a follower for others.

**Real-world scenarios interviewers ask about:**

- *"What happens when a broker goes down?"* - Other brokers automatically take over the failed broker's responsibilities
  through leader election. The cluster continues operating seamlessly.
- *"How do you decide how many brokers you need?"* - Consider factors like throughput requirements, fault tolerance (
  need at least 3 for production), and data retention needs.
- *"Can brokers have different hardware specifications?"* - Yes, but it's not recommended as it creates hotspots.
  Uniform hardware ensures predictable performance.

**Key insight**: Brokers are stateless - they don't store cluster state information. This makes scaling and maintenance
much easier compared to traditional databases.

---

### 🔹 2. **Topic**

* A **topic** is a logical category to which records (messages) are published.
* Topics are **multi-subscriber** – you can have many consumers for one topic.
* Topics are broken down into **partitions**, which are the unit of parallelism.

**Common interview questions:**

- *"How do you design topic naming conventions?"* - Use hierarchical naming like `user.events.login`,
  `payment.transactions.completed`. This helps with organization and access control.
- *"Should you create many small topics or few large topics?"* - Generally prefer fewer, well-organized topics. Too many
  topics create operational overhead and make consumer group management complex.
- *"How do you handle schema evolution in topics?"* - Use schema registry with Avro/JSON schemas. Plan for backward and
  forward compatibility from day one.

**Real-world consideration**: Topics are immutable by design. You can't modify existing messages, only append new ones.
This is crucial for audit trails and event sourcing patterns.

---

### 🔹 3. **Partition**

* A **partition** is a **physically ordered, immutable log**.
* Each message in a partition is assigned a unique **offset**.
* Messages are always **appended** to the end of a partition.
* Kafka **only guarantees order within a single partition**.
* Partitions allow Kafka to:
    * Scale horizontally (across brokers)
    * Provide high throughput via parallelism

> Example: Topic `orders` with 3 partitions can be split across 3 brokers.

**Deep dive concepts interviewers love:**

- *"How do you choose the number of partitions?"* - Start with your expected peak throughput divided by the throughput
  per partition (usually 10-100 MB/s). Factor in the number of consumers you want to run in parallel. Remember: you can
  increase partitions but never decrease them.

- *"What's the relationship between partitions and parallelism?"* - Each partition can only be consumed by one consumer
  within a consumer group. So if you have 12 partitions, you can have at most 12 consumers working in parallel. More
  consumers than partitions means some will be idle.

- *"How does message ordering work?"* - Kafka only guarantees ordering within a partition, not across partitions. If you
  need global ordering, use a single partition (but sacrifice scalability). For most use cases, partition-level ordering
  is sufficient.

**Partitioning strategy deep dive:**

- **Key-based partitioning**: Messages with the same key always go to the same partition. Perfect for user-specific data
  where you need per-user ordering.
- **Round-robin**: When no key is provided, messages are distributed evenly across partitions. Good for load balancing.
- **Custom partitioning**: You can write custom partitioners for special business logic.

---

### 🔹 4. **Leader and Replicas**

* **Each partition has a leader** and 0+ replicas.
* The leader handles **all reads and writes** for that partition.
* Other replicas are **followers**, which replicate data from the leader.
* This setup allows Kafka to remain available and consistent.

> If the leader broker dies, a new leader is elected from in-sync replicas (ISR).

**Leader-Follower Pattern Deep Dive:**
Every partition has one leader and multiple followers (replicas). The leader handles all client interactions while
followers just replicate data.

**Why this design?**

- **Consistency**: Only one broker makes decisions for each partition
- **Performance**: No complex coordination protocols during normal operations
- **Fault tolerance**: If leader fails, one of the followers automatically becomes the new leader

---

### 🔹 5. **Replication & ISR (In-Sync Replicas)**

* Kafka ensures fault tolerance using **replication**.
* A partition's data is replicated across multiple brokers (configurable via `replication.factor`).
* ISR = Set of replicas that are fully caught up with the leader.
* **Follower replicas** continuously fetch data from the leader.
* If a follower falls behind, it is removed from the ISR.
* **Only replicas in the ISR are eligible to be leaders**.

**Unclean leader election** (if enabled) allows out-of-sync replicas to become leader → potential data loss.

**In-Sync Replicas (ISR) - The Critical Concept:**
ISR is the set of replicas that are "close enough" to the leader. This is crucial for understanding Kafka's durability
guarantees.

**Real-world scenarios:**

- *"What happens if an ISR becomes too small?"* - If ISR shrinks below `min.insync.replicas`, producers with `acks=all`
  will start getting errors. This prevents data loss at the cost of availability.
- *"How do you handle the trade-off between consistency and availability?"* - Configure `min.insync.replicas` and
  producer `acks` setting based on your requirements. Financial systems might require `min.insync.replicas=3` and
  `acks=all`, while analytics might use `acks=1`.

---

### 🔹 6. **ZooKeeper vs. KRaft**

#### ZooKeeper (Legacy Mode)

* Kafka traditionally used **Apache ZooKeeper** for:
    * Metadata management
    * Broker registration
    * Controller election
    * Topic configuration

**What ZooKeeper did:**

- Stored which brokers are alive and their metadata
- Managed partition leader elections
- Stored topic configurations and ACLs
- Coordinated consumer group membership

**Why it became a problem:**

- **Operational complexity**: Two systems to manage instead of one
- **Scalability limits**: ZooKeeper ensemble typically 3-5 nodes, became bottleneck for huge clusters
- **Split-brain scenarios**: Network partitions could cause both ZooKeeper and Kafka issues

#### KRaft (Kafka Raft Mode)

* From **Kafka 2.8 onward**, Kafka supports **KRaft mode** (ZooKeeper-less).
* Uses the **Raft consensus algorithm** for metadata quorum.
* Faster, more reliable, less operational complexity.
* Goal: **fully remove ZooKeeper** in future Kafka versions.

> For new clusters, prefer **KRaft mode**.

**Key advantages interviewers ask about:**

- *"How does KRaft improve operations?"* - Single system to deploy, monitor, and troubleshoot. Faster startup times and
  simpler disaster recovery.
- *"What about scalability improvements?"* - KRaft can handle millions of partitions compared to ZooKeeper's hundreds of
  thousands limit.
- *"Is it production ready?"* - Yes, since Kafka 3.3, but migration from ZooKeeper requires planning.

---

## 🔸 Producer Lifecycle

1. **Create producer instance** with configs (e.g. bootstrap servers).
2. Serialize message key and value (e.g. Avro, JSON, Protobuf).
3. Choose partition:
    * Default: hash of key
    * Or user-defined partitioner
4. Send request to broker (async or sync)
5. Wait for acknowledgment based on `acks`:
    * `acks=0`: fire and forget
    * `acks=1`: leader-only ack
    * `acks=all`: wait for all ISR replicas (stronger durability)
6. Optionally retry, batch, or compress messages.

> **Idempotent producers** prevent duplicate messages during retries.

### The Journey of a Message - Deep Dive

Understanding how producers work internally is crucial for performance tuning and troubleshooting.

**The Producer's Internal Flow:**

1. **Application sends message** → Your code calls `producer.send()`
2. **Serialization** → Converts your objects to bytes (JSON, Avro, etc.)
3. **Partitioning decision** → Determines which partition receives the message
4. **Batching** → Groups messages for efficiency (this is key for performance)
5. **Network send** → Actual transmission to brokers
6. **Acknowledgment** → Broker confirms receipt

**Batching and Performance:**
Producers don't send every message immediately. They batch messages for the same partition to improve throughput. This
is why Kafka can handle millions of messages per second.

**Acknowledgment Levels (`acks`):**

- `acks=0`: Fire and forget (fastest, least reliable)
- `acks=1`: Wait for partition leader confirmation (balanced)
- `acks=all`: Wait for all ISR replicas (slowest, most reliable)

**Real-world trade-offs:**

- High-frequency trading: Might use `acks=0` for speed
- Financial transactions: Definitely use `acks=all`
- Analytics data: `acks=1` is usually sufficient

### Producer Error Handling

*"How do you handle producer failures?"* - This is a common troubleshooting question.

**Retriable errors**: Network timeouts, leader elections, temporary broker unavailability
**Non-retriable errors**: Message too large, authentication failures, serialization errors

**Idempotent producers**: Enable `enable.idempotence=true` to prevent duplicate messages during retries. This is crucial
for exactly-once semantics.

---

## 🔸 Consumer Lifecycle

1. **Join a consumer group**.
2. Kafka assigns partitions to consumers in the group.
3. Consume messages from assigned partitions.
4. Keep track of the **offset** (either:
    * **Auto-commit** (less reliable)
    * **Manual commit** (preferred in most cases)
5. Handle rebalancing events (when group membership changes).
6. Acknowledged messages are only consumed once **per group**.

> Multiple consumer groups can independently read the same topic.

### Consumer Groups - Distributed Processing

Consumer groups are how Kafka enables horizontal scaling of message processing.

**The Rebalancing Process:**
When consumers join or leave a group, Kafka redistributes partition assignments. This is called rebalancing.

**Real-world rebalancing scenarios:**

- New consumer joins → Partitions redistributed for better load balancing
- Consumer crashes → Its partitions reassigned to remaining consumers
- New partitions added → Assignment recalculated

**Rebalancing challenges:**

- **Stop-the-world**: During rebalancing, all consumers stop processing
- **Assignment strategies**: Range, round-robin, sticky - each has trade-offs
- **Consumer lag**: Rebalancing can cause temporary processing delays

### Offset Management - Tracking Progress

Offsets are how Kafka tracks which messages each consumer group has processed.

**Offset commit strategies:**

- **Auto-commit**: Convenient but can lead to message loss or duplication
- **Manual commit**: More control but requires careful error handling
- **Sync vs Async commits**: Trade-off between latency and guarantee

**Interview scenarios:**

- *"What happens if a consumer crashes before committing offsets?"* - Messages get reprocessed by another consumer, so
  your application needs to be idempotent.
- *"How do you handle consumer lag?"* - Scale up consumers (up to partition count), optimize processing logic, or
  increase retention time.

---

## 🔸 Real-World Architecture Patterns

### Fan-out Pattern

One producer, multiple consumer groups processing the same data differently.
Example: User activity → Real-time analytics + Batch processing + Audit logging

### Stream Processing Pipeline

Topics feeding into each other through stream processors.
Example: Raw events → Enriched events → Aggregated metrics → Dashboard updates

### Event Sourcing

Using Kafka as the source of truth for all state changes.
Example: Banking transactions stored as immutable events, account balances computed by replaying events.

---

## 🔸 Common Production Challenges (Interview Gold)

### Hot Partitions

*"What causes hot partitions and how do you fix them?"*

- **Cause**: Poor key distribution (e.g., all messages have same key)
- **Solution**: Better partitioning strategy, custom partitioner, or random distribution

### Consumer Lag

*"How do you monitor and handle consumer lag?"*

- **Monitoring**: Track lag metrics per partition and consumer group
- **Solutions**: Scale consumers, optimize processing, increase retention

### Data Loss vs Duplication

*"How do you prevent data loss?"*

- Use `acks=all` and `min.insync.replicas > 1`
- Handle producer retries properly with idempotent producers
- Be prepared for the availability trade-off

### Schema Evolution

*"How do you handle changing data formats?"*

- Use schema registry for centralized schema management
- Design for backward/forward compatibility
- Version your schemas properly

---

## 🔸 Data Flow Summary

```
Producer → Broker (Topic, Partition) → Consumer
       |        |    Partition Leader + Replicas
       |        → ZooKeeper (or KRaft for metadata)
```

---

## 🔸 Summary Table

| Concept       | Description                            |
|---------------|----------------------------------------|
| **Broker**    | Kafka server managing topic partitions |
| **Topic**     | Logical stream of messages             |
| **Partition** | Log file; unit of parallelism          |
| **Producer**  | Sends messages to a topic              |
| **Consumer**  | Reads messages from a topic            |
| **Leader**    | Primary partition owner                |
| **Replica**   | Backup copy of partition               |
| **ISR**       | In-sync replicas with leader           |
| **ZooKeeper** | (Legacy) Cluster metadata              |
| **KRaft**     | Kafka-native metadata system           |

---

## ❓ Interview Questions

1. What happens when the leader of a partition fails?
2. How does Kafka guarantee message ordering?
3. What are the pros and cons of enabling unclean leader election?
4. Why is partitioning important in Kafka?
5. What is the role of ISR in data consistency and availability?
6. How do you choose the number of partitions for a topic?
7. What's the difference between ZooKeeper and KRaft mode?
8. How do you handle hot partitions in production?
9. What are the trade-offs between different producer `acks` settings?
10. How does consumer group rebalancing work and what challenges does it create?
