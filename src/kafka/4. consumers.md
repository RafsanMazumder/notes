# ‚úÖ Kafka Interview Notes: 4. Consumers & Consumer Groups

---

## üî∑ Overview

A **Kafka consumer** is a client application that reads records from topics. Kafka's consumer model is **pull-based** (
i.e., consumers poll for new data) and built to support **horizontal scaling** through **consumer groups** and *
*partition assignment**.

**Interview insight**: *The pull-based model is fundamental to Kafka's scalability. Unlike push-based systems where
brokers decide when to send data, consumers control their consumption rate, enabling better backpressure handling and
allowing consumers to process at their own pace.*

---

## üî∏ 1. Kafka Consumer Basics

* Consumers read records from one or more **partitions** of a topic.
* Records are read **sequentially** from an **offset** (a per-partition record ID).
* Kafka **does not push data** ‚Äì consumers must **poll** regularly to receive data.

### Pull vs Push Model Deep Dive:

**Why Pull-based?**

- *"What are the advantages of pull over push?"* - Consumers control their consumption rate, can handle backpressure
  naturally, and can batch requests efficiently
- **Backpressure handling**: If consumer is slow, it simply polls less frequently rather than overwhelming its buffers
- **Batch efficiency**: Consumers can request many messages in one poll, reducing network overhead
- **Consumer autonomy**: Each consumer can have different processing speeds without affecting others

**Consumer Poll Loop Pattern:**

```java
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        process(record);
    }
    consumer.commitSync(); // Manual commit after processing
}
```

**Advanced Pull Concepts:**

- *"What happens if poll() is not called frequently enough?"* - Consumer may be removed from group due to
  `max.poll.interval.ms` timeout
- *"How does Kafka handle slow consumers?"* - Consumer lag increases, but other consumers in group continue processing
  their partitions
- **Poll timeout tuning**: Short timeouts for responsive applications, longer timeouts for batch processing

---

## üî∏ 2. Consumer Groups

A **consumer group** is a **set of consumers that work together** to consume data from a topic.

* Kafka assigns **each partition to only one consumer** within a group.
* Multiple consumers can exist across different groups and **read the same data** independently.
* Consumers in the same group **share the work** of reading from the topic.

#### Example:

```
Topic: orders (6 partitions)
Consumer Group A:
  - Consumer 1 ‚Üí partitions 0, 1, 2
  - Consumer 2 ‚Üí partitions 3, 4, 5

Consumer Group B:
  - Consumer X ‚Üí partitions 0‚Äì5 (reads all)
```

> This allows Kafka to implement **horizontal scalability** for consumers.

### Consumer Group Deep Dive (Critical Interview Topic):

**Consumer Group Coordinator:**

- Each consumer group has a designated broker as its coordinator
- Coordinator manages group membership and partition assignments
- *"How is coordinator chosen?"* - Based on hash of group ID, ensures even distribution across brokers

**Group Membership Protocol:**

- Consumers send heartbeats to coordinator to maintain membership
- `session.timeout.ms`: Max time without heartbeat before considered dead
- `heartbeat.interval.ms`: Frequency of heartbeat messages (usually 1/3 of session timeout)

**Assignment Strategies Deep Dive:**

**RangeAssignor (Default):**

```
Topic with 7 partitions, 3 consumers:
Consumer 1: partitions 0, 1, 2
Consumer 2: partitions 3, 4
Consumer 3: partitions 5, 6
```

- *Problem*: Uneven distribution, especially with multiple topics

**RoundRobinAssignor:**

```
Consumer 1: partitions 0, 3, 6
Consumer 2: partitions 1, 4
Consumer 3: partitions 2, 5
```

- Better distribution but can cause issues if consumers have different processing capabilities

**StickyAssignor:**

- Maintains existing assignments when possible
- Minimizes partition movement during rebalancing
- *"Why is stickiness important?"* - Preserves consumer state, reduces rebalancing overhead

**Static Group Membership (Kafka 2.3+):**

```java
props.put("group.instance.id", "consumer-1"); // Static ID
```

- Consumer keeps same identity across restarts
- Avoids rebalancing for planned restarts
- *"When would you use static membership?"* - Stream processing applications where rebalancing is expensive

### Multi-Consumer Group Patterns:

**Fan-out Pattern:**

- Same data consumed by multiple groups for different purposes
- Example: Orders topic ‚Üí [Real-time dashboard, Batch analytics, Audit logging]

**Hierarchical Processing:**

- Multiple stages of processing with different consumer groups
- Example: Raw events ‚Üí Enrichment ‚Üí Aggregation ‚Üí Storage

**Disaster Recovery:**

- Separate consumer groups in different data centers
- Primary processes normally, secondary takes over during failures

---

## üî∏ 3. Partition Rebalancing

Partition rebalancing happens when:

* A new consumer **joins** or **leaves** the group
* A consumer **crashes**
* The **number of partitions changes**

Rebalancing involves:

1. **Pausing all consumers** in the group
2. **Reassigning partitions** to consumers
3. **Resuming consumption**

#### Side Effects:

* Rebalancing is a **disruptive** operation
* **Offsets** must be preserved to avoid duplication or loss

### Rebalancing Deep Dive (High-Frequency Interview Topic):

**Rebalancing Triggers in Detail:**

**Consumer Join:**

- New consumer sends JoinGroup request to coordinator
- Coordinator waits for `rebalance.timeout.ms` for all consumers to join
- *"What happens if a consumer is slow to join?"* - It gets excluded from the group until next rebalance

**Consumer Leave/Crash:**

- Graceful leave: Consumer sends LeaveGroup request
- Crash detection: Coordinator notices missing heartbeats after `session.timeout.ms`
- *"How do you distinguish between slow consumer and crashed consumer?"* - Session timeout vs max poll interval timeout

**Consumer Processing Timeout:**

- `max.poll.interval.ms`: Max time between poll() calls
- If exceeded, consumer is considered failed even if sending heartbeats
- *"Why separate heartbeat and poll timeouts?"* - Heartbeat shows consumer is alive, poll timeout shows it's actually
  processing

**Rebalancing Protocol Steps:**

1. **Stop the world**: All consumers stop processing
2. **Join Group**: Consumers request group membership
3. **Leader Election**: Coordinator selects group leader (usually first consumer)
4. **Assignment**: Leader calculates partition assignment
5. **Sync Group**: Assignment distributed to all consumers
6. **Resume**: Consumers start processing assigned partitions

**Rebalancing Performance Impact:**

- Processing stops during rebalancing (typically 1-30 seconds)
- Consumer lag increases during rebalancing pause
- Frequency directly impacts application availability

### üîπ Cooperative Rebalancing (Kafka 2.4+)

Kafka introduced **cooperative sticky assignor** to improve rebalancing.

* Allows **incremental rebalancing**:
    * Only partitions that need to move are reassigned
    * No full stop of all consumers

```java
props.put("partition.assignment.strategy", "CooperativeStickyAssignor");
```

‚úÖ Faster rebalancing
‚úÖ Reduced consumer downtime
‚úÖ Better for long-running apps or stream processing

**Cooperative Rebalancing Implementation:**

- Consumers continue processing partitions they keep
- Only consumers losing/gaining partitions pause briefly
- Typical rebalancing time: 100-500ms vs 5-30 seconds

**Migration to Cooperative Rebalancing:**

- Can't mix eager and cooperative strategies in same group
- Requires coordinated deployment or rolling upgrade strategy
- *"How do you migrate from eager to cooperative rebalancing?"* - Use dual strategy during transition period

**Real-world rebalancing scenarios:**

- *"Black Friday traffic spike requires scaling from 3 to 12 consumers. What happens?"* - Multiple rebalancing rounds as
  consumers join, brief processing pauses, eventual even distribution
- *"Kubernetes pod restart during business hours. How do you minimize impact?"* - Use cooperative rebalancing + static
  group membership + graceful shutdown

---

## üî∏ 4. Offset Management

An **offset** is a pointer to the consumer's current position in a partition.

* Kafka **does not automatically delete records after reading** ‚Äì the consumer must **track its offset**.
* Offsets are stored:
    * In a **special Kafka topic** (`__consumer_offsets`)
    * Or **externally** (e.g., DB, Redis, Zookeeper)

### Offset Storage Deep Dive:

**Internal Offset Storage (`__consumer_offsets`):**

- Compacted topic storing latest offset for each consumer group + partition
- Replication factor typically same as cluster (usually 3)
- *"Why use a Kafka topic to store offsets?"* - Leverages Kafka's durability and replication guarantees
- Automatic cleanup of old consumer group metadata

**External Offset Storage:**

- Store offsets in application database alongside processed results
- Enables atomic commit of processing + offset update
- *"When would you use external offset storage?"* - When you need exactly-once processing with external systems (
  database writes + offset commits)

**Offset Commit Strategies:**

### üîπ Auto vs Manual Commit

#### üü¢ Auto Commit (default)

* Offsets are committed **periodically** (default: every 5 seconds)

```java
enable.auto.commit=true
auto.commit.interval.ms=5000
```

* Simpler but risky:
    * May **lose messages** (if commit before processing)
    * May **reprocess messages** (if crash before commit)

**Auto Commit Scenarios:**

- *"Consumer polls 100 messages, processes 50, auto-commit happens, then crashes. What occurs?"* - 50 messages are lost
  because offsets were committed before processing
- *"When is auto commit acceptable?"* - Fire-and-forget scenarios, analytics where some data loss is acceptable,
  idempotent processing

#### üî¥ Manual Commit

* You control **when offsets are committed**, ensuring **processing is complete**

```java
enable.auto.commit=false
consumer.commitSync(); // or commitAsync()
```

> Best practice: Use **manual commit** for applications that care about **processing guarantees**.

**Manual Commit Patterns:**

**Per-Batch Commit:**

```java
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        process(record);
    }
    consumer.commitSync(); // Commit after processing all records in batch
}
```

**Per-Message Commit (Expensive):**

```java
for (ConsumerRecord<String, String> record : records) {
    process(record);
    consumer.commitSync(Collections.singletonMap(
        new TopicPartition(record.topic(), record.partition()),
        new OffsetAndMetadata(record.offset() + 1)
    ));
}
```

**Atomic Processing + Commit:**

```java
// Begin database transaction
dbTransaction.begin();
try {
    for (ConsumerRecord<String, String> record : records) {
        processToDatabase(record, dbTransaction);
    }
    dbTransaction.commit();
    consumer.commitSync(); // Only commit offset after DB commit
} catch (Exception e) {
    dbTransaction.rollback();
    // Don't commit offset - will reprocess
}
```

### üîπ Commit Types

| Method          | Description                          |
|-----------------|--------------------------------------|
| `commitSync()`  | Blocks until broker confirms         |
| `commitAsync()` | Non-blocking, may skip failures      |
| Combined        | Try async, fallback to sync on close |

**Commit Strategy Deep Dive:**

**Synchronous Commit:**

- Blocks until offset is persisted
- Retries automatically on failure
- Guarantees offset is committed before proceeding
- *"Performance impact"*: Adds latency to processing loop

**Asynchronous Commit:**

- Returns immediately, commits in background
- No automatic retries (could commit out-of-order offsets)
- Higher throughput but less durability guarantee
- *"When do you use async commit?"* - High-throughput scenarios where occasional reprocessing is acceptable

**Hybrid Approach (Production Pattern):**

```java
try {
    while (true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
        process(records);
        consumer.commitAsync(); // Fast async commit during normal operation
    }
} finally {
    consumer.commitSync(); // Ensure final offset is committed on shutdown
}
```

**Offset Commit Failures:**

- `CommitFailedException`: Another consumer in group committed offsets (rebalancing occurred)
- `RebalanceInProgressException`: Rebalancing happening, retry after completion
- Network errors: Automatic retry for sync, manual handling for async

---

## üî∏ Offset Reset Policy

Controls what to do if no previous offset is found:

```java
auto.offset.reset=earliest | latest | none
```

| Value      | Behavior                           |
|------------|------------------------------------|
| `earliest` | Start from beginning of partition  |
| `latest`   | Start from end (new messages only) |
| `none`     | Throw error if no offset is found  |

### Offset Reset Scenarios (Common Interview Topic):

**When Offset Reset is Triggered:**

- New consumer group (no previous offsets)
- Offset committed is beyond current partition end
- Offset committed is before current retention start (data deleted)

**Reset Policy Decision Matrix:**

- **earliest**: Batch processing, data migration, ensuring no data loss
- **latest**: Real-time applications, notifications, live metrics
- **none**: Explicit error handling, ensure conscious decision about data

**Advanced Offset Management:**

**Seeking to Specific Offsets:**

```java
// Seek to beginning
consumer.seekToBeginning(Arrays.asList(partition));

// Seek to specific offset
consumer.seek(partition, specificOffset);

// Seek by timestamp
Map<TopicPartition, Long> timestampsToSearch = Map.of(partition, timestamp);
Map<TopicPartition, OffsetAndTimestamp> offsets = consumer.offsetsForTimes(timestampsToSearch);
```

**Consumer Reset Tools:**

```bash
# Reset consumer group to earliest
kafka-consumer-groups.sh --bootstrap-server localhost:9092 \
  --group my-group --reset-offsets --to-earliest --topic my-topic --execute

# Reset to specific timestamp
kafka-consumer-groups.sh --bootstrap-server localhost:9092 \
  --group my-group --reset-offsets --to-datetime 2024-01-01T00:00:00.000 \
  --topic my-topic --execute
```

---

## üî∏ Advanced Consumer Patterns

### Message Filtering and Processing Patterns

**Client-Side Filtering:**

```java
for (ConsumerRecord<String, String> record : records) {
    if (shouldProcess(record)) {
        process(record);
    }
    // Still commit offset even for filtered messages
}
```

**Batch Processing Pattern:**

```java
List<ConsumerRecord<String, String>> batch = new ArrayList<>();
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    batch.addAll(records);
    
    if (batch.size() >= BATCH_SIZE || shouldFlushBatch()) {
        processBatch(batch);
        consumer.commitSync();
        batch.clear();
    }
}
```

### Error Handling Patterns

**Dead Letter Topic Pattern:**

```java
for (ConsumerRecord<String, String> record : records) {
    try {
        process(record);
    } catch (Exception e) {
        sendToDeadLetterTopic(record, e);
    }
}
```

**Retry with Backoff:**

```java
for (ConsumerRecord<String, String> record : records) {
    int retries = 0;
    while (retries < MAX_RETRIES) {
        try {
            process(record);
            break;
        } catch (RetriableException e) {
            retries++;
            Thread.sleep(BACKOFF_MS * retries);
        }
    }
}
```

### Multi-Threading Patterns

**Single Consumer, Multiple Worker Threads:**

```java
ExecutorService executor = Executors.newFixedThreadPool(WORKER_THREADS);
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        executor.submit(() -> process(record));
    }
    // Note: Offset management becomes complex with async processing
}
```

**Multiple Consumers (Preferred):**

- Each consumer runs in separate thread
- Simpler offset management
- Better fault isolation

---

## üî∏ Consumer Lifecycle Summary

1. Consumer joins a group
2. Kafka assigns partitions to consumers
3. Consumer polls data, processes records
4. Commits offset (auto or manual)
5. If rebalancing occurs ‚Üí partitions may be reassigned

### Detailed Consumer Lifecycle:

**Startup Sequence:**

1. **Configuration**: Set bootstrap servers, group ID, deserializers
2. **Subscription**: Subscribe to topics or assign specific partitions
3. **Group Join**: Send JoinGroup request to coordinator
4. **Partition Assignment**: Receive partition assignment from coordinator
5. **Initial Positioning**: Seek to last committed offset or apply reset policy

**Normal Operation:**

1. **Poll Loop**: Continuously poll for new records
2. **Processing**: Process received records
3. **Offset Management**: Commit offsets (auto or manual)
4. **Heartbeat**: Send periodic heartbeats to coordinator

**Shutdown Sequence:**

1. **Graceful Stop**: Stop polling and processing
2. **Final Commit**: Commit any pending offsets
3. **Leave Group**: Send LeaveGroup request
4. **Close Resources**: Close consumer and cleanup

**Failure Scenarios:**

- **Processing Timeout**: Consumer removed from group, partitions reassigned
- **Network Failure**: Automatic reconnection and rejoin
- **Coordinator Failure**: Find new coordinator and rejoin
- **Broker Failure**: Automatic metadata refresh and continued operation

---

## üî∏ Consumer Performance and Tuning

### Key Performance Configurations:

**Fetch Settings:**

- `fetch.min.bytes`: Minimum data per fetch request
- `fetch.max.wait.ms`: Maximum wait time for fetch request
- `max.partition.fetch.bytes`: Maximum data per partition per fetch

**Poll Settings:**

- `max.poll.records`: Maximum records returned per poll
- `max.poll.interval.ms`: Maximum time between polls

**Session Management:**

- `session.timeout.ms`: Consumer failure detection time
- `heartbeat.interval.ms`: Heartbeat frequency

### Performance Tuning Guidelines:

**High Throughput:**

- Increase `fetch.min.bytes` and `max.poll.records`
- Use larger batches for processing
- Consider async commit for faster processing

**Low Latency:**

- Decrease `fetch.max.wait.ms`
- Use smaller `max.poll.records`
- Process records individually rather than batching

**Reliability:**

- Use manual commits with sync
- Set appropriate session timeout vs heartbeat interval
- Implement proper error handling and retry logic

---

## üî∏ Summary Table

| Feature               | Description                                     |
|-----------------------|-------------------------------------------------|
| Consumer Group        | Allows multiple consumers to share load         |
| Partition Assignment  | Each partition assigned to 1 consumer           |
| Rebalancing           | Redistribution of partitions on topology change |
| Auto Commit           | Offsets committed periodically, riskier         |
| Manual Commit         | More control and reliability                    |
| Cooperative Rebalance | Minimizes consumer pause                        |
| Static Membership     | Avoids rebalancing on planned restarts          |
| Offset Reset          | Behavior when no previous offset exists         |

---

## ‚ùì Interview Questions

### Basic Level:

1. How does Kafka assign partitions to consumers in a group?
2. What triggers a partition rebalance?
3. What is the difference between `commitSync` and `commitAsync`?
4. What are the trade-offs between auto and manual offset commits?
5. How does cooperative rebalancing improve availability?

### Intermediate Level:

6. What happens if a consumer takes longer than `max.poll.interval.ms` to process messages?
7. How do you handle duplicate message processing in consumers?
8. What's the difference between session timeout and poll timeout?
9. How do you implement exactly-once processing with Kafka consumers?
10. What are the trade-offs of different partition assignment strategies?

### Advanced Level:

11. Design a consumer architecture for processing 1 million messages per second.
12. How would you implement a dead letter queue pattern with Kafka?
13. What's your strategy for handling consumer lag in production?
14. How do you migrate a consumer group to a different offset without downtime?
15. Explain the trade-offs between single-threaded and multi-threaded consumer patterns.

### System Design Level:

16. Design a consumer system that processes financial transactions with exactly-once guarantees.
17. How would you implement consumer failover across multiple data centers?
18. What's your approach to schema evolution in consumer applications?
19. Design a monitoring strategy for consumer performance and health.
20. How do you handle consumer group rebalancing in a high-availability system?

### Troubleshooting Level:

21. Consumer group is experiencing frequent rebalancing. How do you diagnose and fix it?
22. Consumer lag is increasing but consumers appear healthy. What could be wrong?
23. How do you recover from a situation where all consumers in a group have crashed?
24. What causes consumer offset commits to fail and how do you handle it?
25. How do you debug performance issues in consumer applications?

---

Understanding consumer behavior is crucial because consumers are where most application logic resides, and consumer
configuration directly impacts data consistency, performance, and system reliability. Most production issues involve
consumer lag, rebalancing problems, or incorrect offset management.