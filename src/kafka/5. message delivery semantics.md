# ✅ Kafka Interview Notes: 5. Kafka Message Delivery Semantics

---

## 🔷 Overview

Kafka offers **three delivery guarantees** based on how you configure the **producer**, **broker**, and **consumer**:

| Semantics     | Description                                    | Risk                                     |
|---------------|------------------------------------------------|------------------------------------------|
| At-most-once  | Messages may be lost, but never duplicated     | Lost messages                            |
| At-least-once | Messages are never lost, but may be duplicated | Duplicate messages                       |
| Exactly-once  | No duplicates, no message loss                 | Requires coordination between components |

**Interview insight**: *Delivery semantics are not just configuration settings - they represent fundamental trade-offs
between consistency, availability, and performance. Understanding when and how to apply each semantic is crucial for
designing reliable distributed systems.*

---

## 🔸 1. At-most-once Delivery

**"Fire and forget"** – messages are sent but no retries.

### Characteristics:

* Producer **does not retry** on failure
* **Auto-commit** offsets **before** processing (on the consumer side)
* If crash happens → messages are **lost**

### Config:

```properties
acks=0
retries=0
enable.auto.commit=true
```

### Use Cases:

* Non-critical logs or telemetry
* Metrics where loss is tolerable

### At-Most-Once Deep Dive:

**Producer-Side Implementation:**

- Producer sends message and immediately considers it delivered
- No acknowledgment wait, no retry mechanism
- Network failures or broker crashes result in silent message loss
- *"Why would anyone choose message loss?"* - Maximum performance, lowest latency, suitable for high-volume,
  non-critical data

**Consumer-Side Implementation:**

- Offsets committed before message processing
- If consumer crashes during processing, message is lost (offset already advanced)
- Auto-commit typically used for simplicity

**Real-world scenarios:**

- **IoT sensor data**: Losing occasional temperature readings is acceptable
- **Website analytics**: Missing some page views won't affect business decisions
- **High-frequency trading feeds**: Latest price matters more than every price update
- **Application logs**: Some log loss is preferable to impacting application performance

**Performance characteristics:**

- Highest throughput (no waiting for acks)
- Lowest latency (fire-and-forget)
- Minimal resource usage
- *"When is at-most-once the right choice?"* - When the business cost of occasional data loss is less than the cost of
  reduced performance

**Configuration mistakes to avoid:**

- Using at-most-once for critical business data
- Not monitoring data loss rates
- Assuming "some loss is okay" without quantifying acceptable loss rates

---

## 🔸 2. At-least-once Delivery

**"Deliver no matter what"** – safe from loss, but may cause duplicates.

### Characteristics:

* Producer **retries** on failures
* Consumer **commits offsets manually** after processing
* Retry from producer or failure after commit → **duplicate delivery**

### Config:

```properties
acks=all
retries > 0
enable.auto.commit=false
```

And:

```java
consumer.commitSync() // after processing message
```

### Use Cases:

* Payment systems (with deduplication logic)
* Notifications
* Stream processing

### At-Least-Once Deep Dive (Most Common in Production):

**Producer-Side Duplicate Scenarios:**

1. **Network timeout after broker receives message**: Producer retries, broker gets duplicate
2. **Leader election during send**: Producer retries to new leader, may duplicate
3. **Acknowledgment lost**: Broker persists message but ack lost in network

**Consumer-Side Duplicate Scenarios:**

1. **Crash after processing, before commit**: Reprocessing same messages after restart
2. **Rebalancing during processing**: New consumer gets already-processed messages
3. **Commit failure**: Processing succeeds but offset commit fails

**Advanced At-Least-Once Patterns:**

**Retry Configuration Best Practices:**

```properties
retries=Integer.MAX_VALUE
delivery.timeout.ms=120000  # 2 minutes total time
retry.backoff.ms=100        # Start with 100ms, exponential backoff
request.timeout.ms=30000    # 30 seconds per request
```

**Consumer Commit Strategies:**

```java
// Pattern 1: Commit per batch
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(100);
    for (ConsumerRecord<String, String> record : records) {
        processMessage(record); // Must be idempotent
    }
    consumer.commitSync(); // Batch commit
}

// Pattern 2: Commit with specific offsets
Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
for (ConsumerRecord<String, String> record : records) {
    processMessage(record);
    offsets.put(
        new TopicPartition(record.topic(), record.partition()),
        new OffsetAndMetadata(record.offset() + 1)
    );
}
consumer.commitSync(offsets);
```

**Duplicate Detection Strategies:**

1. **Message-level IDs**: Include unique ID in each message for deduplication
2. **Offset tracking**: Store last processed offset externally for exact replay
3. **Business-level keys**: Use natural business keys for idempotent operations
4. **Timestamp windows**: Process only messages within time windows

**Real-world at-least-once scenarios:**

- **E-commerce orders**: Better to process duplicate orders (can be reversed) than miss orders
- **Financial notifications**: Users prefer duplicate notifications over missed ones
- **Data pipelines**: ETL processes that can handle duplicate data with upserts
- **Audit logs**: Critical that all events are captured, duplicates can be filtered

**Performance considerations:**

- Higher latency due to retries and ack waiting
- Higher resource usage for duplicate detection
- More complex error handling logic required

---

## 🔸 3. Exactly-once Semantics (EOS)

Kafka supports **true exactly-once** delivery from version **0.11+**, across **producers, brokers, and consumers**.

### Goals:

✅ No duplicate messages
✅ No lost messages
✅ Consistent, one-time processing

### Requirements:

#### ➤ Producer:

* Must enable **idempotence**:

```properties
enable.idempotence=true
acks=all
```

* For multi-topic or partition writes, use **transactions**:

```properties
transactional.id=my-transactional-producer
```

#### ➤ Consumer:

* Must use **read-process-write-commit** logic
* Only commit offsets **after transaction succeeds**

#### ➤ Transaction APIs:

```java
producer.initTransactions();
producer.beginTransaction();
// produce messages
producer.send(...)
producer.commitTransaction();
```

Kafka handles **atomic writes** and **offset commits** inside transactions.

### Exactly-Once Deep Dive (Advanced Interview Topic):

**Two Levels of Exactly-Once:**

**1. Idempotent Producer (Single Partition/Session):**

- Prevents duplicates within single producer session
- Uses Producer ID (PID) + sequence numbers
- Limited to single partition, single producer session
- *"Limitation"*: New producer instance gets new PID, doesn't prevent cross-session duplicates

**2. Transactional Exactly-Once (Multi Partition/Topic):**

- Atomic writes across multiple partitions/topics
- Includes offset commits in transactions
- Survives producer failures and restarts
- *"Use case"*: Stream processing that reads from one topic and writes to another

**Transaction Implementation Details:**

**Transaction Coordinator:**

- Special Kafka component managing transaction state
- Stores transaction metadata in `__transaction_state` topic
- Implements two-phase commit protocol
- *"How many transaction coordinators?"* - One per broker, sharded by transactional.id hash

**Transaction Flow:**

1. **InitTransactions**: Producer registers with coordinator, gets PID
2. **BeginTransaction**: Coordinator marks transaction as started
3. **AddPartitionsToTxn**: Register all partitions that will be written to
4. **Produce**: Send messages with transaction context
5. **AddOffsetsToTxn**: Include consumer offsets in transaction
6. **CommitTransaction**: Two-phase commit across all partitions

**Consumer Integration with Transactions:**

```java
// Consumer configuration for exactly-once
props.put("isolation.level", "read_committed");

// Only see committed messages
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(100);
    // These records are guaranteed to be from committed transactions
}
```

**Exactly-Once Stream Processing Pattern:**

```java
// Read-Process-Write-Commit pattern
producer.beginTransaction();
try {
    ConsumerRecords<String, String> records = consumer.poll(100);
    
    for (ConsumerRecord<String, String> record : records) {
        // Process and produce to output topic
        String result = processMessage(record.value());
        producer.send(new ProducerRecord<>("output-topic", result));
    }
    
    // Include consumer offsets in transaction
    Map<TopicPartition, OffsetAndMetadata> offsets = getOffsetsToCommit(records);
    producer.sendOffsetsToTransaction(offsets, consumer.groupMetadata());
    
    producer.commitTransaction();
} catch (Exception e) {
    producer.abortTransaction();
}
```

**EOS Limitations and Trade-offs:**

**Performance Impact:**

- 2-3x slower than at-least-once due to coordination overhead
- Higher latency due to two-phase commit protocol
- Increased memory usage for transaction state management

**Operational Complexity:**

- More complex monitoring and troubleshooting
- Transaction timeout management
- Coordinator failover handling
- *"When does transaction timeout?"* - `transaction.timeout.ms` (default 60 seconds)

**Scale Limitations:**

- Transaction coordinator can become bottleneck
- Limited by number of concurrent transactions per coordinator
- Cross-datacenter transactions not supported

**Real-world EOS scenarios:**

- **Banking systems**: Account transfers must be atomic (debit + credit)
- **Inventory management**: Stock updates must be exactly-once to prevent overselling
- **Kafka Streams applications**: Stateful processing requiring exactly-once guarantees
- **Data integration**: ETL processes where duplicate data causes incorrect aggregations

**EOS Anti-patterns:**

- Using EOS for high-throughput, loss-tolerant workloads
- Enabling EOS without understanding performance implications
- Mixing transactional and non-transactional producers in same application

---

## 🔸 4. Idempotent Consumers

Even with **at-least-once delivery**, **consumer-side duplication** is possible.

To make consumers **idempotent**, ensure:

1. **Processing is side-effect free** (e.g., no duplicate DB writes)
2. Use **deduplication logic**, such as:
    * Storing last processed **message ID** or **Kafka offset**
    * Upsert in DB using keys (e.g., primary key overwrite)
    * Checkpointing with external systems

#### Example:

```sql
INSERT INTO orders(id, amount)
VALUES (1234, 50) ON CONFLICT (id) DO NOTHING;
```

Or in application code:

```java
if (!seenMessageIds.contains(msg.id)) {
    process(msg);
    seenMessageIds.add(msg.id);
}
```

> ⚠️ Idempotency must be handled **at business logic level** for consumers.

### Idempotent Consumer Patterns (Critical for Production):

**Database-Level Idempotency:**

**Upsert Pattern:**

```sql
-- PostgreSQL
INSERT INTO user_events (user_id, event_type, timestamp, data)
VALUES ($1, $2, $3, $4) ON CONFLICT (user_id, event_type, timestamp) 
DO
UPDATE SET data = EXCLUDED.data;

-- MySQL
INSERT INTO user_events (user_id, event_type, timestamp, data)
VALUES (?, ?, ?, ?) ON DUPLICATE KEY
UPDATE data =
VALUES (data);
```

**Conditional Insert Pattern:**

```sql
INSERT INTO payments (payment_id, amount, status)
SELECT $1,
       $2,
       'PENDING' WHERE NOT EXISTS (
    SELECT 1 FROM payments WHERE payment_id = $1
    );
```

**Application-Level Idempotency:**

**Message ID Tracking:**

```java
// Redis-based deduplication
public boolean isProcessed(String messageId) {
    return redisTemplate.hasKey("processed:" + messageId);
}

public void markProcessed(String messageId) {
    redisTemplate.opsForValue().set("processed:" + messageId, "true", Duration.ofHours(24));
}
```

**Offset-Based Deduplication:**

```java
// Store last processed offset per partition
Map<Integer, Long> lastProcessedOffsets = loadLastProcessedOffsets();

for (ConsumerRecord<String, String> record : records) {
    int partition = record.partition();
    long offset = record.offset();
    
    if (offset <= lastProcessedOffsets.getOrDefault(partition, -1L)) {
        continue; // Already processed
    }
    
    processMessage(record);
    lastProcessedOffsets.put(partition, offset);
    saveLastProcessedOffsets(lastProcessedOffsets);
}
```

**Business Logic Idempotency:**

**State-Based Processing:**

```java
// Example: User balance updates
public void updateUserBalance(String userId, BigDecimal amount, String transactionId) {
    // Check if transaction already applied
    if (transactionExists(transactionId)) {
        return; // Already processed
    }
    
    // Apply update and record transaction
    updateBalance(userId, amount);
    recordTransaction(transactionId, userId, amount);
}
```

**Versioned Updates:**

```java
// Example: Document updates with version control
public void updateDocument(String docId, String content, long expectedVersion) {
    Document doc = getDocument(docId);
    if (doc.getVersion() >= expectedVersion) {
        return; // Already updated to this version or newer
    }
    
    doc.setContent(content);
    doc.setVersion(expectedVersion);
    saveDocument(doc);
}
```

**Advanced Idempotency Patterns:**

**Distributed Deduplication:**

- Use distributed cache (Redis Cluster) for message ID tracking
- Implement consensus algorithms for duplicate detection
- Use external coordination services (ZooKeeper, etcd)

**Time-Window Deduplication:**

- Only check for duplicates within recent time window
- Balances memory usage with duplicate detection accuracy
- Useful for high-volume, time-sensitive data

**Checkpointing Pattern:**

```java
// Process in batches with checkpoints
List<Message> batch = new ArrayList<>();
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(100);
    batch.addAll(extractMessages(records));
    
    if (batch.size() >= BATCH_SIZE) {
        String checkpointId = processIdempotentBatch(batch);
        commitBatchCheckpoint(checkpointId, getLastOffset(records));
        batch.clear();
    }
}
```

---

## 🔸 5. Delivery Semantics Summary

| Guarantee     | Producer Config                         | Consumer Behavior                        | Risk                     |
|---------------|-----------------------------------------|------------------------------------------|--------------------------|
| At-most-once  | `acks=0`, `retries=0`                   | Auto-commit before processing            | Message loss             |
| At-least-once | `acks=all`, `retries>0`                 | Manual commit after processing           | Duplicates possible      |
| Exactly-once  | `enable.idempotence=true`, Transactions | Transactional commit of offset + message | Safe, needs coordination |

### Advanced Configuration Combinations:

**High-Performance At-Least-Once:**

```properties
# Producer
acks=1
retries=Integer.MAX_VALUE
enable.idempotence=true
batch.size=65536
linger.ms=10
# Consumer
enable.auto.commit=false
max.poll.records=1000
```

**Bulletproof Exactly-Once:**

```properties
# Producer
enable.idempotence=true
acks=all
retries=Integer.MAX_VALUE
transactional.id=unique-id-per-instance
transaction.timeout.ms=60000
# Consumer
isolation.level=read_committed
enable.auto.commit=false
```

**High-Throughput At-Most-Once:**

```properties
# Producer
acks=0
retries=0
batch.size=262144
linger.ms=100
compression.type=snappy
# Consumer
enable.auto.commit=true
auto.commit.interval.ms=1000
```

---

## 🔸 Best Practices

| Use Case                                      | Suggested Semantics                 |
|-----------------------------------------------|-------------------------------------|
| Logging, metrics                              | At-most-once                        |
| Payment, billing, orders                      | At-least-once + idempotent consumer |
| Banking, inventory updates                    | Exactly-once (with transactions)    |
| Stream processing (e.g. Flink, Kafka Streams) | Exactly-once                        |

### Extended Best Practices:

**Choosing the Right Semantic:**

**At-Most-Once is appropriate when:**

- High throughput is more important than data completeness
- Business can tolerate occasional data loss
- Processing cost of duplicates exceeds cost of missing data
- System resources are constrained

**At-Least-Once is appropriate when:**

- Data loss is unacceptable but duplicates can be handled
- Downstream systems are naturally idempotent
- Performance is important but not critical
- Moderate complexity is acceptable

**Exactly-Once is appropriate when:**

- Both data loss and duplicates are unacceptable
- System can tolerate increased latency and complexity
- Strong consistency is required
- Audit and compliance requirements are strict

**Mixed Semantics Architecture:**

```
High-volume metrics → At-most-once → Analytics DB
User events → At-least-once → Event processing + dedup
Financial transactions → Exactly-once → Core banking system
```

---

## 🔸 Common Pitfalls and Solutions

### Producer-Side Pitfalls:

**Infinite Retries Without Timeout:**

- Problem: Producer retries forever on transient errors
- Solution: Set `delivery.timeout.ms` for overall timeout

**Mixed Idempotent/Non-Idempotent Producers:**

- Problem: Some producers idempotent, others not in same application
- Solution: Standardize on idempotent producers for consistency

### Consumer-Side Pitfalls:

**Processing Before Commit vs Commit Before Processing:**

- Problem: Wrong order leads to loss or duplicates
- Solution: Always process first, then commit (at-least-once)

**Ignoring Rebalancing Impact:**

- Problem: Rebalancing can cause reprocessing of uncommitted messages
- Solution: Implement proper rebalancing listeners and state management

### System-Level Pitfalls:

**Mismatched Producer/Consumer Semantics:**

- Problem: Exactly-once producer with at-most-once consumer
- Solution: Align semantics across entire data pipeline

**Not Monitoring Delivery Guarantees:**

- Problem: Semantic violations go undetected
- Solution: Monitor duplicate rates, loss rates, and processing lag

---

## 🔸 Monitoring and Observability

### Key Metrics for Each Semantic:

**At-Most-Once Monitoring:**

- Message loss rate (compare sent vs received)
- Producer error rate without retries
- Consumer processing rate vs incoming rate

**At-Least-Once Monitoring:**

- Duplicate detection rate
- Retry rate and retry latency
- Consumer lag and rebalancing frequency

**Exactly-Once Monitoring:**

- Transaction success/abort rate
- Transaction duration and timeout rate
- Coordinator availability and failover events

### Production Alerting:

**Critical Alerts:**

- Consumer lag exceeding threshold
- High producer retry rate
- Transaction abort rate above normal
- Offset commit failure rate

**Warning Alerts:**

- Duplicate detection rate increasing
- Rebalancing frequency above normal
- Message loss detected (for at-most-once)

---

## ❓ Interview Questions

### Basic Level:

1. What's the difference between at-least-once and exactly-once semantics?
2. How does Kafka provide idempotent delivery at the producer level?
3. Can consumers be exactly-once without transactional producers?
4. How would you implement an idempotent consumer?
5. What's the risk of enabling auto-commit in consumers?

### Intermediate Level:

6. Explain the trade-offs between different delivery semantics.
7. How do transactions work in Kafka's exactly-once implementation?
8. What are the performance implications of exactly-once semantics?
9. How do you handle duplicate messages in at-least-once delivery?
10. What's the role of the transaction coordinator in EOS?

### Advanced Level:

11. Design a payment processing system with exactly-once guarantees.
12. How would you implement cross-service exactly-once processing?
13. What are the limitations of Kafka's exactly-once semantics?
14. How do you monitor and alert on delivery semantic violations?
15. Explain the interaction between consumer isolation levels and transactions.

### System Design Level:

16. Design a multi-tenant system with different delivery guarantees per tenant.
17. How would you migrate from at-least-once to exactly-once without downtime?
18. What's your approach to exactly-once processing across multiple Kafka clusters?
19. How do you handle exactly-once semantics in a microservices architecture?
20. Design a disaster recovery strategy that maintains delivery guarantees.

### Troubleshooting Level:

21. Producer shows successful sends but consumer never receives messages. Diagnose the issue.
22. Exactly-once processing is slower than expected. What could be wrong?
23. Consumer is processing duplicate messages despite proper configuration. Debug steps?
24. Transaction timeouts are increasing. How do you investigate and resolve?
25. How do you verify that your exactly-once implementation is actually working correctly?

---

Understanding delivery semantics is crucial because they define the fundamental guarantees your system provides to
users. The choice between semantics affects everything from performance to data consistency, and most production issues
can be traced back to mismatched expectations about delivery guarantees.